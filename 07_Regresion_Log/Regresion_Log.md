## Aprendizaje Supervisado - Regresion Logistica - Clasificaci√≥n

El aprendizaje supervisado consiste en encontrar una funci√≥n ‚Ñé (hip√≥tesis) que aproxime la funci√≥n real ùëì(ùë•) a partir de un conjunto de entrenamiento con pares de entrada-salida. Esta hip√≥tesis se elige dentro de un espacio de hip√≥tesis ùêª, que puede incluir funciones lineales, polinomiales, l√≥gicas, entre otras.

<p align="center">
<img src="img/modelo.png" width="500">
</p>

La funci√≥n resultante es utilizada posteriormente para predecir valores a partir de ejemplos de datos no etiquetados.

### Selecci√≥n de conjunto de datos

El conjunto de datos son pares de datos etiquetados, en este caso un analista indicar√° en funci√≥n a la experiencia el etiquetado de datos.

P.e. en el caso conjunto de datos para la detecci√≥n de correos Spam, el conjunto de datos ser√≠a de la siguiente forma.

|email(x) |etiqueta(y)|
|---------|-----------|
|email1   |1          |
|email2   |0          |
|email3   |1          |
|...      |...        |

donde:
x = variables de entrada
y = variables de salida
(x,y) =  ejemplo de entrenamiento

0 : Clase Negativa (Correo legitimo)
1 : Clase Positiva (Correo Spam)



### Representacion
- Se puede basar en conocimiento previo o en un an√°lisis exploratorio de datos (gr√°ficos, pruebas estad√≠sticas).
- Se pueden probar diferentes espacios y evaluar cu√°l se ajusta mejor.


<p align="center">
<img src="img/modelo_spam.png" width="300">
</p>


Ejemplo:

Sup√≥n que tienes este modelo:

$$P(y=1‚à£x) = \frac{1}{1 + e^{(‚àí3+1.5x)}}$$

Ejemplo:
Si x es el numero de de tag html

    Si x=0, entonces:
    z=‚àí3+1.5(0)=‚àí3‚áíprobabilidad‚âà0.047(muybaja)
    z=‚àí3+1.5(0)=‚àí3‚áíprobabilidad‚âà0.047(muybaja)

    Si x=2, entonces:
    z=‚àí3+1.5(2)=0‚áíprobabilidad=0.5
    z=‚àí3+1.5(2)=0‚áíprobabilidad=0.5

    Si x=4, entonces:
    z=‚àí3+1.5(4)=3‚áíprobabilidad‚âà0.95
    z=‚àí3+1.5(4)=3‚áíprobabilidad‚âà0.95

## Funci√≥n de hip√≥tesis

La funci√≥n logit fue introducida por Joseph Berkson en el a√±o 1944. √âl era un bioestad√≠stico que trabajaba en la Cl√≠nica Mayo en EE. UU. Berkson us√≥ la funci√≥n logit como alternativa a otro m√©todo de an√°lisis para variables binarias, en especial en contextos m√©dicos.

Propuso el uso de la funci√≥n:

$$
logit(p) = log\left(\frac{p}{1 - p}\right)
$$

como una forma de transformar probabilidades para que pudieran analizarse con modelos lineales.

Berkson se bas√≥ en el concepto de odds (probabilidades relativas), que ya se usaban en estad√≠stica m√©dica. El trabajo previo sobre la funci√≥n sigmoide o curva log√≠stica, que Pierre Fran√ßois Verhulst hab√≠a usado en 1838 para modelar crecimiento poblacional. T√©cnicas estad√≠sticas como la m√°xima verosimilitud, que comenzaron a desarrollarse con fuerza en los a√±os 1930-40.

__Origen de la funci√≥n log√≠stica__

La funci√≥n log√≠stica proviene del campo de la biolog√≠a poblacional en el siglo XIX. Fue introducida por Pierre Fran√ßois Verhulst en 1838 como una forma de modelar el crecimiento de poblaciones.

üîπ Problema original
Verhulst not√≥ que las poblaciones:
- Crecen r√°pido al inicio (crecimiento exponencial).
- Pero luego se ralentizan por recursos limitados (comida, espacio).
- Finalmente, se estabilizan en un l√≠mite m√°ximo (llamado "capacidad de carga").

Este comportamiento se modela con la ecuaci√≥n log√≠stica diferencial:

$$
\frac{dP}{dt} = rP\left(1 - \frac{P}{K} \right)
$$

- ùëÉ: poblaci√≥n.
- ùëü: tasa de crecimiento.
- ùêæ: capacidad m√°xima.

La soluci√≥n de esa ecuaci√≥n es la funci√≥n log√≠stica:

$$
P(t) = \frac{K}{1 + Ae^{-rt}}
$$

- ùëÉ(ùë°): poblaci√≥n en el tiempo ùë°.
- ùêæ: capacidad m√°xima del entorno (o poblaci√≥n l√≠mite).
- ùê¥: constante relacionada con la condici√≥n inicial.
- ùëü: tasa de crecimiento.
- $e^{-rt}$  : decaimiento exponencial.


Aunque Berkson introdujo el nombre ‚Äúlogit‚Äù, la regresi√≥n log√≠stica como modelo general se fue desarrollando en paralelo por otros estad√≠sticos en los a√±os siguientes. Se consolid√≥ en los a√±os 1960 y 70 como herramienta clave en epidemiolog√≠a, ciencias sociales y aprendizaje autom√°tico.

Relaci√≥n con Bernoulli
El modelo logit parte de la distribuci√≥n Bernoulli, que describe experimentos con dos posibles resultados: √©xito (1) o fracaso (0).

Al aplicar m√°xima verosimilitud sobre datos binarios, se obtiene una funci√≥n que lleva directamente a la forma de la regresi√≥n log√≠stica, en donde el logit aparece naturalmente.

__En resumen__

Joseph Berkson (1944) acu√±√≥ el t√©rmino logit y propuso usarlo para modelar probabilidades.
Se bas√≥ en:
- La funci√≥n log√≠stica usada desde el siglo XIX (Verhulst).
- La necesidad de modelar relaciones no lineales entre variables y probabilidades.
- Los fundamentos de la estad√≠stica y la probabilidad (como Bernoulli).

__Explicacion__
Regresi√≥n log√≠stica
https://www.youtube.com/watch?v=82Hxn6hu_P4

1. Funci√≥n log√≠stica (sigmoide):

$$
P(y|x) = \frac{e^{\theta_{0}+\theta_{1}x}}{1 + e^{\theta_{0}+\theta_{1}x}}
$$

Esto representa la probabilidad de que y=1y=1 dado xx. Es la salida de una regresi√≥n log√≠stica, y el resultado siempre est√° entre 0 y 1.

<p align="center">
<img src="img/funcion_logistica.png" width="500">
</p>

2. Transformaci√≥n logit (log-odds):

$$
ln \left(\frac{P(y|x)}{1 - P(y|x)}\right)=\theta_{0}+\theta_{1}x
$$

Esto es simplemente aplicar la funci√≥n logit a la probabilidad P(y‚à£x). Convierte la probabilidad en "log-odds" (logaritmo de las razones de probabilidades), que puede tomar cualquier valor real.

3. Definici√≥n de logit:

$$
logit(p) = log\left(\frac{p}{1 - p}\right)
$$

<p align="center">
<img src="img/funcion_logit.png" width="500">
</p>


En tu caso, est√°s usando p=P(y‚à£x), as√≠ que:

$$
logit(P(y|x)) = \theta_{0}+\theta_{1}x
$$


Conexi√≥n clave

Lo que hace la regresi√≥n log√≠stica es modelar los log-odds como una funci√≥n lineal de las variables independientes xx. Luego, al aplicar la funci√≥n log√≠stica inversa (la sigmoide), se convierte ese valor en una probabilidad entre 0 y 1.


Ejemplo:
```
# Importar librer√≠as necesarias
import numpy as np
import matplotlib.pyplot as plt
```

```
# Funci√≥n sigmoide
def sigmoid(x, theta_0, theta_1):
    return 1 / (1 + np.exp(-(theta_0 + theta_1 * x)))
```
```
# Funci√≥n para graficar sigmoide y logit
def plot_logit_and_sigmoid(theta_0=0.0, theta_1=1.0):
    x = np.linspace(-10, 10, 500)
    logit = theta_0 + theta_1 * x
    prob = sigmoid(x, theta_0, theta_1)

    fig, ax = plt.subplots(2, 1, figsize=(8, 10))

    # Gr√°fico de la sigmoide
    ax[0].plot(x, prob, label='Sigmoide', color='blue')
    ax[0].set_title('Funci√≥n log√≠stica (sigmoide)')
    ax[0].set_xlabel('x')
    ax[0].set_ylabel('Probabilidad')
    ax[0].grid(True)
    ax[0].legend()
    ax[0].axhline(0.5, color='gray', linestyle='--', linewidth=0.8)
    ax[0].axvline(0, color='gray', linestyle='--', linewidth=0.8)

    # Gr√°fico del logit
    ax[1].plot(x, logit, label='Logit (log-odds)', color='red')
    ax[1].set_title('Funci√≥n logit')
    ax[1].set_xlabel('x')
    ax[1].set_ylabel('Log-odds')
    ax[1].grid(True)
    ax[1].legend()
    ax[1].axhline(0, color='gray', linestyle='--', linewidth=0.8)
    ax[1].axvline(0, color='gray', linestyle='--', linewidth=0.8)

    plt.tight_layout()
    plt.show()
```
```
plot_logit_and_sigmoid(theta_0=1, theta_1=1)
```

Refencias

- The elemens of Statistical Learning 

https://drive.google.com/file/d/1VWYHo30-lzof_kDVHdxJ8tMMmYsBY3vY/view?usp=drive_link

- An Intoduction to Statical Leaning

https://drive.google.com/file/d/1nU9HisQVA1A16u0j4mGu9RWJ80-RCITR/view?usp=drive_link

- Metodos Predictivos de Aprendizaje Estadistico

https://rubenfcasal.github.io/aprendizaje_estadistico/m%C3%A9todos-de-aprendizaje-estad%C3%ADstico.html