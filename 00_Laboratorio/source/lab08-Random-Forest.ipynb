{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/drive/1eQOkYB4XMQdSeHV_pKvt9kSUW8vQo6LQ\">Abre este Jupyter en Google Colab</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Logística: Detección de SPAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio se muestran los fundamentos de la Regresión Logística planteando uno de los primeros problemas que fueron solucionados mediante el uso de técnicas de Machine Learning: la detección de SPAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enunciado del ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se propone la construcción de un sistema de aprendizaje automático capaz de predecir si un correo determinado se corresponde con un correo de SPAM o no, para ello, se utilizará el siguiente conjunto de datos:\n",
    "\n",
    "##### [2007 TREC Public Spam Corpus](https://plg.uwaterloo.ca/cgi-bin/cgiwrap/gvcormac/foo07)\n",
    "The corpus trec07p contains 75,419 messages:\n",
    "\n",
    "    25220 ham\n",
    "    50199 spam\n",
    "\n",
    "These messages constitute all the messages delivered to a particular\n",
    "server between these dates:\n",
    "\n",
    "    Sun, 8 Apr 2007 13:07:21 -0400\n",
    "    Fri, 6 Jul 2007 07:04:53 -0400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de librerías externas\n",
    "!pip install scikit-learn\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Funciones complementarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso práctico relacionado con la detección de correos electrónicos de SPAM, el conjunto de datos que disponemos esta formado por correos electrónicos, con sus correspondientes cabeceras y campos adicionales. Por lo tanto, requieren un preprocesamiento previo a que sean ingeridos por el algoritmo de Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta clase facilita el preprocesamiento de correos electrónicos que poseen código HTML\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs = True\n",
    "        self.fed = []\n",
    "\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta función se encarga de elimar los tags HTML que se encuentren en el texto del correo electrónico\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de eliminación de los tags HTML de un texto\n",
    "t = '<tr><td align=\"left\"><a href=\"../../issues/51/16.html#article\">Phrack World News</a></td>'\n",
    "strip_tags(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además de eliminar los posibles tags HTML que se encuentren en el correo electrónico, deben realizarse otras acciones de preprocesamiento para evitar que los mensajes contengan ruido innecesario. Entre ellas se encuentra la eliminación de los signos de puntuación, eliminación de posibles campos del correo electrónico que no son relevantes o eliminación de los afijos de una palabra manteniendo únicamente la raiz de la misma (Stemming). La clase que se muestra a continuación realiza estas transformaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class Parser:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stemmer = nltk.PorterStemmer()\n",
    "        self.stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "        self.punctuation = list(string.punctuation)\n",
    "\n",
    "    def parse(self, email_path):\n",
    "        \"\"\"Parse an email.\"\"\"\n",
    "        with open(email_path, errors='ignore') as e:\n",
    "            msg = email.message_from_file(e)\n",
    "        return None if not msg else self.get_email_content(msg)\n",
    "\n",
    "    def get_email_content(self, msg):\n",
    "        \"\"\"Extract the email content.\"\"\"\n",
    "        subject = self.tokenize(msg['Subject']) if msg['Subject'] else []\n",
    "        body = self.get_email_body(msg.get_payload(),\n",
    "                                   msg.get_content_type())\n",
    "        content_type = msg.get_content_type()\n",
    "        # Returning the content of the email\n",
    "        return {\"subject\": subject,\n",
    "                \"body\": body,\n",
    "                \"content_type\": content_type}\n",
    "\n",
    "    def get_email_body(self, payload, content_type):\n",
    "        \"\"\"Extract the body of the email.\"\"\"\n",
    "        body = []\n",
    "        if type(payload) is str and content_type == 'text/plain':\n",
    "            return self.tokenize(payload)\n",
    "        elif type(payload) is str and content_type == 'text/html':\n",
    "            return self.tokenize(strip_tags(payload))\n",
    "        elif type(payload) is list:\n",
    "            for p in payload:\n",
    "                body += self.get_email_body(p.get_payload(),\n",
    "                                            p.get_content_type())\n",
    "        return body\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Transform a text string in tokens. Perform two main actions,\n",
    "        clean the punctuation symbols and do stemming of the text.\"\"\"\n",
    "        for c in self.punctuation:\n",
    "            text = text.replace(c, \"\")\n",
    "        text = text.replace(\"\\t\", \" \")\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        tokens = list(filter(None, text.split(\" \")))\n",
    "        # Stemming of the tokens\n",
    "        return [self.stemmer.stem(w) for w in tokens if w not in self.stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lectura de un correo en formato raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parsing del correo electrónico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Parser()\n",
    "p.parse(\"datasets/trec07p/data/inmail.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lectura del índice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas funciones complementarias se encargan cargar en memoria la ruta de cada correo electrónico y su etiqueta correspondiente {spam, ham}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = open(\"datasets/trec07p/full/index\").readlines()\n",
    "##index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATASET_PATH = os.path.join(\"datasets\", \"trec07p\")\n",
    "\n",
    "def parse_index(path_to_index, n_elements):\n",
    "    ret_indexes = []\n",
    "    index = open(path_to_index).readlines()\n",
    "    for i in range(n_elements):\n",
    "        mail = index[i].split(\" ../\")\n",
    "        label = mail[0]\n",
    "        path = mail[1][:-1]\n",
    "        path_mail = path.split(\"/\")[-1]\n",
    "        ret_indexes.append({\"label\":label, \"email_path\":os.path.join(DATASET_PATH, os.path.join(\"data\", path_mail))})\n",
    "    return ret_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_email(index):\n",
    "    p = Parser()\n",
    "    pmail = p.parse(index[\"email_path\"])\n",
    "    return pmail, index[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = parse_index(\"datasets/trec07p/full/index\", 10)\n",
    "indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocesamiento de los datos del conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con las funciones presentadas anteriormente se permite la lectura de los correos electrónicos de manera programática y el procesamiento de los mismos para eliminar aquellos componentes que no resultan de utilidad para la detección de correos de SPAM. Sin embargo, cada uno de los correos sigue estando representado por un diccionario de Python con una serie de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el índice y las etiquetas en memoria\n",
    "index = parse_index(\"datasets/trec07p/full/index\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos el primer correo\n",
    "import os\n",
    "\n",
    "open(index[0][\"email_path\"]).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parseamos el primer correo\n",
    "mail, label = parse_email(index[0])\n",
    "print(\"El correo es:\", label)\n",
    "print(mail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de Regresión Logística no es capaz de ingerir texto como parte del conjunto de datos. Por lo tanto, deben aplicarse una serie de funciones adicionales que transformen el texto de los correos electrónicos parseados en una representación numérica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aplicación de CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Preapración del email en una cadena de texto\n",
    "prep_email = [\" \".join(mail['subject']) + \" \".join(mail['body'])]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit(prep_email)\n",
    "\n",
    "print(\"Email:\", prep_email, \"\\n\")\n",
    "print(\"Características de entrada:\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.transform(prep_email)\n",
    "print(\"\\nValues:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aplicación de OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "prep_email = [[w] for w in mail['subject'] + mail['body']]\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "X = enc.fit_transform(prep_email)\n",
    "\n",
    "print(\"Features:\\n\", enc.get_feature_names_out())\n",
    "print(\"\\nValues:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funciones auxiliares para preprocesamiento del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prep_dataset(index_path, n_elements):\n",
    "    X = []\n",
    "    y = []\n",
    "    indexes = parse_index(index_path, n_elements)\n",
    "    for i in range(n_elements):\n",
    "        print(\"\\rParsing email: {0}\".format(i+1), end='')\n",
    "        try:\n",
    "            mail, label = parse_email(indexes[i])\n",
    "            X.append(\" \".join(mail['subject']) + \" \".join(mail['body']))\n",
    "            y.append(label)\n",
    "        except:\n",
    "            pass\n",
    "    #print(\"X:\", X[:15])\n",
    "    #print(\"y:\", y[:15])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Entrenamiento del algoritmo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos únicamente un subconjunto de 100 correos electrónicos\n",
    "X_train, y_train = create_prep_dataset(\"datasets/trec07p/full/index\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aplicamos la vectorización a los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=20, min_df=2, max_df=0.9)\n",
    "X_train = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##print(X_train.toarray())\n",
    "##print(\"\\nFeatures:\", len(vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(X_train.toarray(), columns=[vectorizer.get_feature_names_out()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Entrenamiento del algoritmo de regresión logística con el conjunto de datos preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lectura de un conjunto de correos nuevos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos 150 correos de nuestro conjunto de datos y nos quedamos únicamente con los 50 últimos \n",
    "# Estos 50 correos electrónicos no se han utilizado para entrenar el algoritmo\n",
    "\n",
    "# Si deseo incrementar el numero de correos\n",
    "# X_train, y_train = create_prep_dataset(\"datasets/trec07p/full/index\", 1000)\n",
    "\n",
    "\n",
    "X_test = X_train[100:]\n",
    "y_test = y_train[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocesamiento de los correos con el vectorizador creado anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predicción del tipo de correo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicción:\\n\", y_pred)\n",
    "print(\"\\nEtiquetas reales:\\n\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluación de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy: {:.3f}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleccion de Caracteristicas con Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga y preprocesamiento de los correos\n",
    "X_train_text, y_train_text = create_prep_dataset(\"datasets/trec07p/full/index\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paso 1: Vectorización completa (todas las palabras)\n",
    "vectorizer = CountVectorizer()\n",
    "X_all = vectorizer.fit_transform(X_train_text)\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Paso 2: Random Forest para selección de características\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_all, y_train_text)\n",
    "\n",
    "# Paso 3: Obtener importancias de características\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Ordenamos por importancia\n",
    "indices = np.argsort(importances)[::-1]\n",
    "top_n = 20  # Número de palabras importantes a seleccionar\n",
    "top_features = feature_names[indices[:top_n]]\n",
    "\n",
    "print(f\"Top {top_n} palabras importantes según Random Forest:\")\n",
    "for i, feature in enumerate(top_features):\n",
    "    print(f\"{i+1}. {feature} ({importances[indices[i]]:.4f})\")\n",
    "\n",
    "# Paso 4: Vectorización solo con top features\n",
    "vectorizer_top = CountVectorizer(vocabulary=top_features)\n",
    "X_top = vectorizer_top.fit_transform(X_train_text)\n",
    "\n",
    "# Paso 5: Entrenar regresión logística con solo esas características\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_top, y_train_text)\n",
    "\n",
    "# Mostrar coeficientes aprendidos\n",
    "print(\"\\nCoeficientes aprendidos por la Regresión Logística:\")\n",
    "for word, coef in zip(top_features, clf.coef_[0]):\n",
    "    print(f\"{word}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizar con caracteristicas seleccionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paso 1: Vectorización completa (todas las palabras)\n",
    "vectorizer = CountVectorizer()\n",
    "X_all = vectorizer.fit_transform(X_train_text)  # X_train_text es tu conjunto de texto\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Paso 2: Random Forest para selección de características\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_all, y_train_text)\n",
    "\n",
    "# Paso 3: Obtener importancias de características\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Ordenamos por importancia y seleccionamos las 29 características más importantes\n",
    "indices = np.argsort(importances)[::-1]\n",
    "top_n = 20  # Seleccionar las 29 características más importantes\n",
    "top_features = feature_names[indices[:top_n]]\n",
    "\n",
    "# Paso 4: Vectorización solo con las 29 características más importantes\n",
    "vectorizer_top = CountVectorizer(vocabulary=top_features)\n",
    "X_top = vectorizer_top.fit_transform(X_train_text)\n",
    "\n",
    "# Ahora X_top contiene solo las 29 características más importantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos un modelo de regresión logística sobre las 29 características seleccionadas\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_top, y_train_text)\n",
    "\n",
    "# Hacemos predicciones\n",
    "y_pred = clf.predict(X_top)\n",
    "\n",
    "# Evaluamos el modelo (por ejemplo, precisión)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_train_text, y_pred)\n",
    "print(f\"Precisión con las 20 características seleccionadas: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reducción de dimensionalidad a 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X_top.toarray())\n",
    "\n",
    "# Graficar en 2D\n",
    "plt.figure(figsize=(10, 6))\n",
    "for label in np.unique(y_train):\n",
    "    idx = np.array(y_train_text) == label\n",
    "    plt.scatter(X_2d[idx, 0], X_2d[idx, 1], label=label, edgecolors='k', s=60, alpha=0.7)\n",
    "\n",
    "plt.title(\"Visualización 2D con las 20 características más importantes\")\n",
    "plt.xlabel(\"Componente principal 1\")\n",
    "plt.ylabel(\"Componente principal 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reducción de dimensionalidad\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X_top.toarray())\n",
    "\n",
    "# Entrenar un clasificador en 2D\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_2d, y_train_encoded)  # y_train_encoded es numérico (0 o 1)\n",
    "\n",
    "# Crear una malla de puntos para graficar la frontera\n",
    "x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1\n",
    "y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n",
    "                     np.linspace(y_min, y_max, 500))\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# Predecir en cada punto de la malla\n",
    "Z = clf.predict(grid_points)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.2, cmap=plt.cm.coolwarm)\n",
    "\n",
    "# Puntos reales\n",
    "colors = ['blue', 'red']\n",
    "for i, label in enumerate(label_encoder.classes_):  # spam / ham\n",
    "    idx = y_train_encoded == i\n",
    "    plt.scatter(X_2d[idx, 0], X_2d[idx, 1], c=colors[i], label=label, edgecolors='k', s=60)\n",
    "\n",
    "plt.title(\"Visualización 2D con línea de separación (Regresión Logística)\")\n",
    "plt.xlabel(\"Componente principal 1\")\n",
    "plt.ylabel(\"Componente principal 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
