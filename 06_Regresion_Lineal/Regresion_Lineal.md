## Aprendizaje Supervisado - Regresion Lineal

<p align="center">
<img src="img/modelo.png" width="500">
</p>

### Conjunto de datos de entrenamiento

|N√∫mero sistema afectado (x)  | Coste en euros (y) |
|-----------------------------|--------------------|
|1000                         | 10,000             |
|1500                         | 20,000             |
|500                          | 5,500              |
|...                          |...                 |

donde:
-  x = variables de entrada
-  y = variables de salida
-  (x,y) = ejemplo de entrenamiento

### Funci√≥n de hip√≥tesis

$$h_{\theta}(x)=\theta_{0}+\theta_{1}x$$


<p align="center">
<img src="img/reg_lineal_incidente.png" width="500">
</p>

La funci√≥n hipotesis de la regresi√≥n lineal, est√° representada por una funci√≥n generica que podria ser una linea recta en el caso de que solo se considere una caracteristica en el modelo.

(*) Si en el conjunto de datos se considerar varias caracteristicas para la evaluaci√≥n del modelo, funci√≥n hipotesis ser√≠a una regresi√≥n lineal multivariable la cual se representa de la siguiente manera:

$$h_{\theta}(x)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+...+\theta_{n}x_{n}$$

__Como medimos el error?__

La diferencia entre el valor real y el valor predicho es:
error·µ¢ = y·µ¢ - ≈∑·µ¢ = y·µ¢ - (m * x·µ¢ + b)

Este error puede ser positivo o negativo. Si solo los sumamos, podr√≠an cancelarse.

Para evitar que los errores se cancelen entre s√≠ y penalizar m√°s los errores grandes, usamos el error cuadr√°tico:
(y·µ¢ - (m * x·µ¢ + b))¬≤

Ahora sumamos todos esos errores para todos los puntos del conjunto de entrenamiento:
Œ£ (y·µ¢ - (m * x·µ¢ + b))¬≤

Y finalmente, como hay n muestras, sacamos el promedio:
ECM(m, b) = (1/n) * Œ£ [y·µ¢ - (m * x·µ¢ + b)]¬≤

__Par√°metros de la funci√≥n__

Los par√°metros de la funci√≥n $$\theta_{0}$$ y $$\theta_{1}$$ , determinan la funci√≥n matematica. El objetivo es encontrar el valor optimo de esos par√°metros que ajusten mejor a la tendencia de nuestros conjuntos de datos.

¬øC√≥mo se minimiza?
Usamos c√°lculo diferencial para minimizarla. Para encontrar los valores √≥ptimos de m y b, derivamos el ECM respecto a m y b, y los igualamos a cero:

1. Derivamos el ECM respecto a m y b:
Derivada respecto a m:

‚àÇECM/‚àÇm = (-2/n) * Œ£ x·µ¢ * [y·µ¢ - (m * x·µ¢ + b)]

Derivada respecto a b:

‚àÇECM/‚àÇb = (-2/n) * Œ£ [y·µ¢ - (m * x·µ¢ + b)]

Derivando la funci√≥n ECM con respecto a ùëö y ùëè, y resolviendo el sistema de ecuaciones que se genera (igualando las derivadas parciales a cero).
Pero vamos paso a paso con el resultado final ya derivado:

F√≥rmulas:

$$m=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}$$

$$b=\bar{y}-\bar{x}*m$$

Donde:

$$\bar{x}$$ : es el promedio de los valores de entrada 

$$\bar{y}$$ : es el promedio de los valores de salida 

https://www.youtube.com/watch?v=hmVh2ddVCK4


### Construcci√≥n del modelo

Para obtener la funci√≥n en un regresion lineal utilizamos los siguientes comandos:

from sklearn.linear_model import LinearRegression

__Construcci√≥n del modelo y ajuste de la funci√≥n hip√≥tesis__
```
lin_reg = LinearRegression()
lin_reg.fit(df['n_equipos_afectados'].values.reshape(-1, 1), df['coste'].values)
```
donde fit entrena al modelo:
üîπ df['n_equipos_afectados']: es la variable independiente (feature)
üîπ df['coste']: es la variable dependiente (target) que se quiere predecir
üîπ .values.reshape(-1, 1): transforma la columna en un array de 2D, requerido por scikit-learn (ya que espera una matriz de entrada, no un vector unidimensional) 

Para obtener estos datos se utilizan los siguientes comandos:

__Par√°metro theta 0__
```
lin_reg.intercept_
```
__Par√°metro theta 1__
```
lin_reg.coef_
```

## Ajuste del modelo

### Funci√≥n de coste - Error Cuadr√°tico Medio

Se debe minimizar una funci√≥n de coste J($$\theta$$), para obtener los par√°metros √≥ptimos.

https://www.youtube.com/watch?v=lkGyu70gAzE

Para graficar la funci√≥n del Error Cuadr√°tico Medio, debemos expresar la funci√≥n en base a la funci√≥n hipotesis de la regresi√≥n.

Dado un conjunto de datos $$(x_{i},y_{i})$$, el modelo lineal $$h_{\theta}(x_{i})=\theta_{0}+\theta_{1}*x_{i}$$

$$
\text{ECM}(m, b) = \frac{1}{n} \sum_{i=1}^{n} \left( y_i - (mx_i + b) \right)^2
$$

Esta funci√≥n es una par√°bola respecto a los par√°metros ùëö y ùëè, porque:

- El ECM es una suma de cuadrados ‚Üí siempre convexa.
- Es una funci√≥n cuadr√°tica respecto a ùëö y ùëè.

Visualizar la parabola del ECM

```
import numpy as np
import matplotlib.pyplot as plt

# Datos de ejemplo
x = np.array([1, 2, 3, 4])
y = np.array([2, 4, 6, 8])

# Exploramos diferentes pendientes m, con b = 0 fijo
m_values = np.linspace(0, 4, 100)
ecm_values = []

for m in m_values:
    y_pred = m * x  # b = 0
    ecm = np.mean((y - y_pred) ** 2)
    ecm_values.append(ecm)

# Graficar la par√°bola de ECM
plt.plot(m_values, ecm_values)
plt.title("Error cuadr√°tico medio vs pendiente m")
plt.xlabel("Pendiente (m)")
plt.ylabel("ECM")
plt.grid(True)
plt.show()
```


### Funci√≥n de Optimizaci√≥n - Gradiente descendente


https://www.youtube.com/watch?v=za61eVtq2MY