## Aprendizaje Supervisado - Regresion Lineal

<p align="center">
<img src="img/modelo.png" width="500">
</p>

### Conjunto de datos de entrenamiento

|N√∫mero sistema afectado (x)  | Coste en euros (y) |
|-----------------------------|--------------------|
|1000                         | 10,000             |
|1500                         | 20,000             |
|500                          | 5,500              |
|...                          |...                 |

donde:
-  x = variables de entrada
-  y = variables de salida
-  (x,y) = ejemplo de entrenamiento

### Funci√≥n de hip√≥tesis

$$h_{\theta}(x)=\theta_{0}+\theta_{1}x$$


<p align="center">
<img src="img/reg_lineal_incidente.png" width="500">
</p>

La funci√≥n hipotesis de la regresi√≥n lineal, est√° representada por una funci√≥n generica que podria ser una linea recta en el caso de que solo se considere una caracteristica en el modelo.

(*) Si en el conjunto de datos se considerar varias caracteristicas para la evaluaci√≥n del modelo, funci√≥n hipotesis ser√≠a una regresi√≥n lineal multivariable la cual se representa de la siguiente manera:

$$h_{\theta}(x)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+...+\theta_{n}x_{n}$$

__Como medimos el error?__

La diferencia entre el valor real y el valor predicho es:
error·µ¢ = y·µ¢ - ≈∑·µ¢ = y·µ¢ - (m * x·µ¢ + b)

Este error puede ser positivo o negativo. Si solo los sumamos, podr√≠an cancelarse.

Para evitar que los errores se cancelen entre s√≠ y penalizar m√°s los errores grandes, usamos el error cuadr√°tico:
(y·µ¢ - (m * x·µ¢ + b))¬≤

Ahora sumamos todos esos errores para todos los puntos del conjunto de entrenamiento:
Œ£ (y·µ¢ - (m * x·µ¢ + b))¬≤

Y finalmente, como hay n muestras, sacamos el promedio:
ECM(m, b) = (1/n) * Œ£ [y·µ¢ - (m * x·µ¢ + b)]¬≤

__Par√°metros de la funci√≥n__

Los par√°metros de la funci√≥n $$\theta_{0}$$ y $$\theta_{1}$$ , determinan la funci√≥n matematica. El objetivo es encontrar el valor optimo de esos par√°metros que ajusten mejor a la tendencia de nuestros conjuntos de datos.

¬øC√≥mo se minimiza?
Usamos c√°lculo diferencial para minimizarla. Para encontrar los valores √≥ptimos de m y b, derivamos el ECM respecto a m y b, y los igualamos a cero:

1. Derivamos el ECM respecto a m y b:
Derivada respecto a m:

‚àÇECM/‚àÇm = (-2/n) * Œ£ x·µ¢ * [y·µ¢ - (m * x·µ¢ + b)]

Derivada respecto a b:

‚àÇECM/‚àÇb = (-2/n) * Œ£ [y·µ¢ - (m * x·µ¢ + b)]

Derivando la funci√≥n ECM con respecto a ùëö y ùëè, y resolviendo el sistema de ecuaciones que se genera (igualando las derivadas parciales a cero).
Pero vamos paso a paso con el resultado final ya derivado:

F√≥rmulas:

$$m=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}$$

$$b=\bar{y}-\bar{x}*m$$

Donde:

$$\bar{x}$$ : es el promedio de los valores de entrada 

$$\bar{y}$$ : es el promedio de los valores de salida 

https://www.youtube.com/watch?v=hmVh2ddVCK4


### Construcci√≥n del modelo

Para obtener la funci√≥n en un regresion lineal utilizamos los siguientes comandos:

from sklearn.linear_model import LinearRegression

__Construcci√≥n del modelo y ajuste de la funci√≥n hip√≥tesis__
```
lin_reg = LinearRegression()
lin_reg.fit(df['n_equipos_afectados'].values.reshape(-1, 1), df['coste'].values)
```
donde fit entrena al modelo:
üîπ df['n_equipos_afectados']: es la variable independiente (feature)
üîπ df['coste']: es la variable dependiente (target) que se quiere predecir
üîπ .values.reshape(-1, 1): transforma la columna en un array de 2D, requerido por scikit-learn (ya que espera una matriz de entrada, no un vector unidimensional) 

Para obtener estos datos se utilizan los siguientes comandos:

__Par√°metro theta 0__
```
lin_reg.intercept_
```
__Par√°metro theta 1__
```
lin_reg.coef_
```
